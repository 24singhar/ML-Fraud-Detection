{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6681d092-ae83-41ff-a5a6-f117cb9a1383",
   "metadata": {},
   "source": [
    "# Fraud Detection Project Summary\n",
    "\n",
    "## Brief Intro\n",
    "In this project, we are addressing the problem of fraud detection in financial transactions, a critical issue for many businesses, especially in the banking and e-commerce sectors. The primary goal is to identify fraudulent activities effectively without causing undue inconvenience to customers conducting legitimate transactions. To this end, it is crucial to design a model that maximizes recall, the ability to catch all fraudulent transactions, without overly sacrificing precision, ensuring that transactions flagged as fraudulent are indeed fraudulent. Therefore, my primary measure for performance evaluation is recall and my secondary measure is precision. Recall can be calculated as the True Positives divided by True Postitives + False Negatives. Precision is equal to True Positives divided by True Positives + False Positives. \n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "I examined the dataset checking the dimensions, columns and null values. The dataset had 416789 rows and 24 columns and zero null values. When examining the columns, I noticed some variables that I did not need including and am few values I wanted to convert to different formats. For example, I just wanted the hour from the transaction timestamp column because I thought the time of day may play an important role in fraud as fraudulent activity may be more likely to occur at unusual hours. For date of birht, I extracted year only as age of the customer as a persons age may impact the likely hood of their card information being stolen or misused. I threw out personal information of the customer except their latitude and longitude as transactions that take place far away from where a person lives may be more likely to be fraudulent. \n",
    "\n",
    "I used the following 9 features to predict fraud:\n",
    "['amount', 'lat', 'long', 'city_pop', 'merch_lat', 'merch_long', 'hour', 'year_of_birth', 'category']\n",
    "\n",
    "To convert categorical variables to numerical variables, I used one hot encoder which converts each unique value into its own binary column. The only variable that needed to be converted when I selected my final features was the category of purchase. I used it with the merchant as well but found that it did not help my models and thus left it out to make my models more efficient. \n",
    "\n",
    "## Classification Model Selection\n",
    "\n",
    "I tested various classification models including Logistic Regression, Support Vector Machines, Gaussian naive Bayes. The three best models I selected are Decision Tree, Random Forest, and Gradient Boosted Trees. My decision tree performed best on nearly all performance measures with recall rate of 70.4% meaning it would detect about 70% of fraudulent transactions and a precision rate around 92.46%. Ranking second place was a little trickier as my random forest model had a much higher precision rate but a slightly lower recall rate. Given the significant difference in precision rate, I decided that the random forest model would be a better fit with a 60.4% recall rate but a 96.6% precision rate. Gradient boosting was 3rd with a recall rate around 64.1% and precision of 80.4%. \n",
    "\n",
    "## Description of classifier and Hyperparameters of each model:\n",
    "Hyperparameters used:\n",
    "1. max_depth: species maximum depth of the tree. Limiting the depth can prevent overfitting of data and improve generalizability.\n",
    "2. min_samples_split: determines the minimum number of samples needed for node to split. This controls the minimum size of the dataset on which decisions to stop excessive splitting of data.\n",
    "3. min_samples_leaf: sets the minimum number of samples a leaf node must have. All this means is that no leaf in the tree will represent less than two samples ensuring nodes are not hyperspecific.\n",
    "4. random_state: meant to make data reproducible. Ensures that each random split is the same and this is used in all the models\n",
    "5. n_estimators: specifies number of trees in a forest (random forest) or models in an ensemble (Graident Boosted Trees). Generally, higher n-estimator numbers would lead to better accuracy but to an extent; however, those reutrns diminish and can eventually have a negative impact on the model \n",
    "6. learning_rate: determines how much to change the model in regard to estimated error each time weights are updates (gradient boosting). Lower learning rates result in slower learning for the model and being more cautious which can be beneficial when finding the optimal solution.\n",
    "\n",
    "In finding optimal hyperparameters, I had to manually adjust the models do to limitations of my computer and it was through analysing the the recall, precision, accuracy, f1-score, and generalizability to the test set that I determined the best parameters.\n",
    "\n",
    "1. Decision Tree:  This is a classification model that consists of a root node with no incoming branches, followed by internal nodes that have both incoming and outgoing branches and terminate into leaf nodes, hence the term decision \"tree\". Each node runs a test on a feature of the data with a binary outcome. Pros of decision trees are its simplicity and interprebility, ability to work with categorical and numerical data (though it is unable to do this in scikit learn), and are generally fast for training and prediction. Some of the cons are overfitting from overly complex trees and can be volatile from small variations in data.\n",
    "\n",
    "Decision Tree Scores:\n",
    "Accuracy: 0.9986\n",
    "Precision: 0.9264\n",
    "Recall: 0.7040\n",
    "F1 Score: 0.8000\n",
    "\n",
    "hyperparameters = (max_depth = 10, min_samples_split - 9, min_samples_leaf = 2, random_state = 42)\n",
    "Meaning: no more than 10 levels of nodes, at least 9 samples to split a node, at least 2 samples to create a leaf node and an arbitrary random_state of 42 to ensure reproducibility of results.\n",
    "\n",
    "2. Random Forest: A random forest classificatoin model combines multiple decision trees to come to a single decision. Each decision tree comprises a sample of the dataset that is drawn with replacement meaning the same data points can be used more than once. One of the key benefits of random forest models is that they are pretty good at preventing overfitting of data. A couple of downsides are that it is time-consuming and can be harder to interpret how decisions were made. \n",
    "\n",
    "Random Forest Scores:\n",
    "Accuracy: 0.9983\n",
    "Precision: 0.9664\n",
    "Recall: 0.6037\n",
    "F1 Score: 0.7432\n",
    "\n",
    "hyperparameters = (random_state = 42)\n",
    "Meaning: Here, I have only put random state and that is because the default values produced the best result. 100 decision trees will be used for averaging predictions (n-estimator), at least 2 samples per split (default), at least one sample per leaf node (default)\n",
    "  \n",
    "3.  Gradient Boosted Trees: This is an ensemble technique that sequentially builds a series of decision trees. Each tree corrects errors of the previous one, which effectively improves the model incrementally. Thus, the trees are dependent the ones that come before unlike with random forest classification. Like random forest, gradient boosted trees can be time consuming but are also more prone to overfitting.\n",
    "\n",
    "Gradient-Boosted Tree Scores:\n",
    "Accuracy: 0.9979\n",
    "Precision: 0.8041\n",
    "Recall: 0.6410\n",
    "F1 Score: 0.7134\n",
    "\n",
    "hyperparameters =  (learning_rate = 0.1, max_depth=3, random_state=42)\n",
    "Meaning: each iteration will correct for 10% of the errors (learning rate) and 3 is the maximum level of nodes. n_estimator is the default value of 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0c912-b623-409f-93af-c5b010790c3e",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "1f5ffb6b-f365-435f-8cb1-0a7c2e1f2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314cbf2-65b2-469f-8d34-a7504a8a0f6a",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c01c93b3-f388-48b7-8139-cda9185ee113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>transaction_timestamp</th>\n",
       "      <th>credit_card_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job_title</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>transaction</th>\n",
       "      <th>POSIX_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119106</td>\n",
       "      <td>119106</td>\n",
       "      <td>02/08/2020 07:55</td>\n",
       "      <td>3.778960e+14</td>\n",
       "      <td>fraud_Bahringer, Schoen and Corkery</td>\n",
       "      <td>shopping_pos</td>\n",
       "      <td>1.07</td>\n",
       "      <td>Kimberly</td>\n",
       "      <td>Myers</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>41.4682</td>\n",
       "      <td>-72.5751</td>\n",
       "      <td>5438</td>\n",
       "      <td>Librarian, academic</td>\n",
       "      <td>17/11/1964</td>\n",
       "      <td>cf581d75ccc9ba838a05dec8bfa78b5b</td>\n",
       "      <td>1375430128</td>\n",
       "      <td>41.240083</td>\n",
       "      <td>-71.837788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179292</td>\n",
       "      <td>179292</td>\n",
       "      <td>23/08/2020 14:05</td>\n",
       "      <td>3.036410e+13</td>\n",
       "      <td>fraud_Romaguera, Wehner and Tromp</td>\n",
       "      <td>kids_pets</td>\n",
       "      <td>94.99</td>\n",
       "      <td>Samuel</td>\n",
       "      <td>Sandoval</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>35.8896</td>\n",
       "      <td>-96.0887</td>\n",
       "      <td>7163</td>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>05/02/1982</td>\n",
       "      <td>b1bfaf13224da41f422db483fd810dd7</td>\n",
       "      <td>1377266716</td>\n",
       "      <td>35.156537</td>\n",
       "      <td>-95.806648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>540729</td>\n",
       "      <td>540729</td>\n",
       "      <td>28/12/2020 16:22</td>\n",
       "      <td>3.032840e+13</td>\n",
       "      <td>fraud_Berge-Hills</td>\n",
       "      <td>kids_pets</td>\n",
       "      <td>31.28</td>\n",
       "      <td>Helen</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0290</td>\n",
       "      <td>-93.1607</td>\n",
       "      <td>602</td>\n",
       "      <td>Cytogeneticist</td>\n",
       "      <td>14/07/1954</td>\n",
       "      <td>cde9fc0136873645778d0ad8817db655</td>\n",
       "      <td>1388247749</td>\n",
       "      <td>39.888665</td>\n",
       "      <td>-93.106804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>374360</td>\n",
       "      <td>374360</td>\n",
       "      <td>14/11/2020 10:44</td>\n",
       "      <td>3.036410e+13</td>\n",
       "      <td>fraud_Connelly, Reichert and Fritsch</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>73.06</td>\n",
       "      <td>Samuel</td>\n",
       "      <td>Sandoval</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>35.8896</td>\n",
       "      <td>-96.0887</td>\n",
       "      <td>7163</td>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>05/02/1982</td>\n",
       "      <td>90b8429191e5c83df1afba4e5db4d61e</td>\n",
       "      <td>1384425890</td>\n",
       "      <td>36.734101</td>\n",
       "      <td>-96.737345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314574</td>\n",
       "      <td>314574</td>\n",
       "      <td>19/10/2020 01:50</td>\n",
       "      <td>4.198470e+12</td>\n",
       "      <td>fraud_Kuphal-Predovic</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>9.99</td>\n",
       "      <td>Christie</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>41.4768</td>\n",
       "      <td>-95.3509</td>\n",
       "      <td>2036</td>\n",
       "      <td>Engineering geologist</td>\n",
       "      <td>20/08/1971</td>\n",
       "      <td>e4893795b6b3e41667129b9ed13b9650</td>\n",
       "      <td>1382147409</td>\n",
       "      <td>40.922072</td>\n",
       "      <td>-94.899388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0 transaction_timestamp  credit_card_num  \\\n",
       "0        119106      119106      02/08/2020 07:55     3.778960e+14   \n",
       "1        179292      179292      23/08/2020 14:05     3.036410e+13   \n",
       "2        540729      540729      28/12/2020 16:22     3.032840e+13   \n",
       "3        374360      374360      14/11/2020 10:44     3.036410e+13   \n",
       "4        314574      314574      19/10/2020 01:50     4.198470e+12   \n",
       "\n",
       "                               merchant       category  amount first_name  \\\n",
       "0   fraud_Bahringer, Schoen and Corkery   shopping_pos    1.07   Kimberly   \n",
       "1     fraud_Romaguera, Wehner and Tromp      kids_pets   94.99     Samuel   \n",
       "2                     fraud_Berge-Hills      kids_pets   31.28      Helen   \n",
       "3  fraud_Connelly, Reichert and Fritsch  gas_transport   73.06     Samuel   \n",
       "4                 fraud_Kuphal-Predovic       misc_net    9.99   Christie   \n",
       "\n",
       "    last_name gender  ...      lat     long city_pop               job_title  \\\n",
       "0       Myers      F  ...  41.4682 -72.5751     5438     Librarian, academic   \n",
       "1    Sandoval      M  ...  35.8896 -96.0887     7163  Fitness centre manager   \n",
       "2    Campbell      F  ...  40.0290 -93.1607      602          Cytogeneticist   \n",
       "3    Sandoval      M  ...  35.8896 -96.0887     7163  Fitness centre manager   \n",
       "4  Williamson      F  ...  41.4768 -95.3509     2036   Engineering geologist   \n",
       "\n",
       "   date_of_birth                       transaction  POSIX_time  merch_lat  \\\n",
       "0     17/11/1964  cf581d75ccc9ba838a05dec8bfa78b5b  1375430128  41.240083   \n",
       "1     05/02/1982  b1bfaf13224da41f422db483fd810dd7  1377266716  35.156537   \n",
       "2     14/07/1954  cde9fc0136873645778d0ad8817db655  1388247749  39.888665   \n",
       "3     05/02/1982  90b8429191e5c83df1afba4e5db4d61e  1384425890  36.734101   \n",
       "4     20/08/1971  e4893795b6b3e41667129b9ed13b9650  1382147409  40.922072   \n",
       "\n",
       "  merch_long is_fraud  \n",
       "0 -71.837788        0  \n",
       "1 -95.806648        0  \n",
       "2 -93.106804        0  \n",
       "3 -96.737345        0  \n",
       "4 -94.899388        0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using one hot encoder to convert categorical variables to be used in models that require numeric variables\n",
    "ohe = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False).set_output(transform = 'pandas')\n",
    "# read in data\n",
    "data = pd.read_csv(\"mis501_fraud.csv\")\n",
    "# first 5 rows with columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cf0d3135-e47c-40cb-a3e4-f5833b976292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416789, 24)\n",
      "Index(['Unnamed: 0.1', 'Unnamed: 0', 'transaction_timestamp',\n",
      "       'credit_card_num', 'merchant', 'category', 'amount', 'first_name',\n",
      "       'last_name', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long',\n",
      "       'city_pop', 'job_title', 'date_of_birth', 'transaction', 'POSIX_time',\n",
      "       'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n",
      "Unnamed: 0.1             0\n",
      "Unnamed: 0               0\n",
      "transaction_timestamp    0\n",
      "credit_card_num          0\n",
      "merchant                 0\n",
      "category                 0\n",
      "amount                   0\n",
      "first_name               0\n",
      "last_name                0\n",
      "gender                   0\n",
      "street                   0\n",
      "city                     0\n",
      "state                    0\n",
      "zip                      0\n",
      "lat                      0\n",
      "long                     0\n",
      "city_pop                 0\n",
      "job_title                0\n",
      "date_of_birth            0\n",
      "transaction              0\n",
      "POSIX_time               0\n",
      "merch_lat                0\n",
      "merch_long               0\n",
      "is_fraud                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# data dimensions\n",
    "print(data.shape)\n",
    "# get column names\n",
    "print(data.columns)\n",
    "#no null_values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "#remove \"fraud_\" from start of each merchant name\n",
    "data['merchant'] = data['merchant'].str.replace('^fraud_', '', regex=True)\n",
    "\n",
    "#adjust format of data in certain columns: get hour from time of transaction and year from date of birth\n",
    "data['hour'] = pd.to_datetime(data['transaction_timestamp'], format='%d/%m/%Y %H:%M').dt.hour\n",
    "data['year_of_birth']=pd.to_datetime(data['date_of_birth'], format='%d/%m/%Y').dt.year\n",
    "\n",
    "#removing variables we do not want\n",
    "new_data = data.drop([\"transaction_timestamp\", \"job_title\", \"merchant\", \"date_of_birth\", \"credit_card_num\", \"POSIX_time\", \"zip\", \"Unnamed: 0.1\", \"Unnamed: 0\", \"first_name\", \"last_name\", \"gender\", \"street\", \"city\", \"state\", \"transaction\"], axis = \"columns\")\n",
    "\n",
    "# encoding the category column using one hot encoder to convert the values to a numeric, in this case a binary value\n",
    "ohetransform = ohe.fit_transform(new_data[['category']])\n",
    "# create new df\n",
    "updated_data = pd.concat([new_data, ohetransform], axis=1).drop(columns = ['category'])\n",
    "\n",
    "updated_data.head()\n",
    "\n",
    "# define target and feature variables\n",
    "predictors = updated_data[[column for column in updated_data.columns if column != 'is_fraud']]\n",
    "\n",
    "# Split the dataset into the training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, updated_data[\"is_fraud\"], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c693f1-cfac-46e6-9b38-176af2da24cd",
   "metadata": {},
   "source": [
    "# 1. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "39abe49c-b315-470d-a908-9890091fde10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9986\n",
      "Precision: 0.9264\n",
      "Recall: 0.7040\n",
      "F1 Score: 0.8000\n",
      "[[103745     24]\n",
      " [   127    302]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    clf_dt = DecisionTreeClassifier(max_depth = 10, min_samples_split = 9, min_samples_leaf = 2, random_state = 42) # shift-tab here to show how much can be changed\n",
    "    dt_predicted = clf_dt.fit(X_train, y_train).predict(X_test)\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    accuracy = accuracy_score(y_test, dt_predicted)\n",
    "    precision = precision_score(y_test, dt_predicted)\n",
    "    recall = recall_score(y_test, dt_predicted)\n",
    "    f1 = f1_score(y_test, dt_predicted)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    cm = confusion_matrix(y_test, dt_predicted)\n",
    "    print(cm)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f1d54-49bb-43b4-9a2f-6132b50bf8bc",
   "metadata": {},
   "source": [
    "# 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "97d0d3e4-fe7c-4c95-8922-9087a4c98b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rf_clf = RandomForestClassifier(random_state = 42)\n",
    "    rf_predicted = rf_clf.fit(X_train, y_train).predict(X_test)\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "695f47b7-5a47-462e-ac43-428b2862568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9983\n",
      "Precision: 0.9664\n",
      "Recall: 0.6037\n",
      "F1 Score: 0.7432\n",
      "[[103760      9]\n",
      " [   170    259]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    accuracy = accuracy_score(y_test, rf_predicted)\n",
    "    precision = precision_score(y_test, rf_predicted)\n",
    "    recall = recall_score(y_test, rf_predicted)\n",
    "    f1 = f1_score(y_test, rf_predicted)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "try:\n",
    "    cm = confusion_matrix(y_test, rf_predicted)\n",
    "    print(cm)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667e734-c94c-44cc-9884-65611bc449f7",
   "metadata": {},
   "source": [
    "# 3. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a1169bd2-2afa-4808-916f-7b5a8394af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gbc_clf = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.1, max_depth=3, random_state=42)\n",
    "    gbc_pred = gbc_clf.fit(X_train, y_train).predict(X_test)\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "2d033338-b567-45c0-ba3c-bc46fe277f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9979\n",
      "Precision: 0.8041\n",
      "Recall: 0.6410\n",
      "F1 Score: 0.7134\n",
      "[[103702     67]\n",
      " [   154    275]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    accuracy = accuracy_score(y_test, gbc_pred)\n",
    "    precision = precision_score(y_test, gbc_pred)\n",
    "    recall = recall_score(y_test, gbc_pred)\n",
    "    f1 = f1_score(y_test, gbc_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    cm = confusion_matrix(y_test, gbc_pred)\n",
    "    print(cm)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
